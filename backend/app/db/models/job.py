"""Job database model for storing scraped and analyzed job listings."""

import uuid
from datetime import datetime
from enum import StrEnum
from typing import TYPE_CHECKING

from sqlalchemy import Boolean, DateTime, Float, ForeignKey, String, Text, UniqueConstraint
from sqlalchemy.dialects.postgresql import JSON, UUID
from sqlalchemy.orm import Mapped, mapped_column, relationship

from app.db.base import Base, TimestampMixin

if TYPE_CHECKING:
    from app.db.models.job_profile import JobProfile
    from app.db.models.user import User


class JobStatus(StrEnum):
    """Status of a job in the user's pipeline.

    Flow: NEW â†’ PREPPED â†’ REVIEWED â†’ APPLIED â†’ INTERVIEWING
    - Can go to REJECTED from APPLIED or INTERVIEWING (employer rejects)
    - To remove a job from listings, use soft delete (sets deleted_at)
    """

    NEW = "new"
    PREPPED = "prepped"
    REVIEWED = "reviewed"
    APPLIED = "applied"
    INTERVIEWING = "interviewing"
    REJECTED = "rejected"  # Employer rejected the application


class Job(Base, TimestampMixin):
    """Job model for storing scraped job listings with analysis results.

    Each job is associated with a user and contains the scraped job data
    plus AI-generated relevance scoring and reasoning.
    """

    __tablename__ = "jobs"
    __table_args__ = (
        # Prevent duplicate job URLs per user
        UniqueConstraint("user_id", "job_url", name="jobs_user_id_job_url_key"),
    )

    id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id: Mapped[uuid.UUID] = mapped_column(
        UUID(as_uuid=True),
        ForeignKey("users.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    # Profile used to search for this job (for prep to use same profile)
    profile_id: Mapped[uuid.UUID | None] = mapped_column(
        UUID(as_uuid=True),
        ForeignKey("job_profiles.id", ondelete="SET NULL"),
        nullable=True,
        index=True,
    )

    # Job details from scraping
    title: Mapped[str] = mapped_column(String(500), nullable=False)
    company: Mapped[str] = mapped_column(String(255), nullable=False)
    location: Mapped[str | None] = mapped_column(String(255), nullable=True)
    description: Mapped[str | None] = mapped_column(Text, nullable=True)
    job_url: Mapped[str] = mapped_column(String(2048), nullable=False)
    salary_range: Mapped[str | None] = mapped_column(String(100), nullable=True)
    date_posted: Mapped[datetime | None] = mapped_column(DateTime(timezone=True), nullable=True)
    source: Mapped[str | None] = mapped_column(
        String(50), nullable=True
    )  # linkedin, indeed, glassdoor, etc.
    ingestion_source: Mapped[str | None] = mapped_column(
        String(20), nullable=True, index=True
    )  # How the job was discovered: 'scrape', 'email', 'manual'

    # AI analysis results
    relevance_score: Mapped[float | None] = mapped_column(Float, nullable=True, index=True)
    reasoning: Mapped[str | None] = mapped_column(Text, nullable=True)

    # User workflow status
    status: Mapped[str] = mapped_column(
        String(50), default=JobStatus.NEW.value, nullable=False, index=True
    )

    # Search metadata
    search_terms: Mapped[str | None] = mapped_column(
        String(500), nullable=True
    )  # Terms used to find this job

    # User notes
    notes: Mapped[str | None] = mapped_column(Text, nullable=True)

    # Prep materials (generated by job_prep pipeline)
    cover_letter: Mapped[str | None] = mapped_column(Text, nullable=True)
    cover_letter_file_path: Mapped[str | None] = mapped_column(
        String(500), nullable=True
    )  # S3 path to generated PDF
    cover_letter_generated_at: Mapped[datetime | None] = mapped_column(
        DateTime(timezone=True), nullable=True
    )
    prep_notes: Mapped[str | None] = mapped_column(
        Text, nullable=True
    )  # Combined highlights and talking points
    prepped_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True), nullable=True)

    # Application analysis (from job_analyze pipeline)
    application_type: Mapped[str | None] = mapped_column(
        String(50), nullable=True
    )  # easy_apply, ats, direct, email, unknown
    application_url: Mapped[str | None] = mapped_column(
        String(2048), nullable=True
    )  # Direct application URL (may differ from job_url)
    requires_cover_letter: Mapped[bool | None] = mapped_column(Boolean, nullable=True)
    requires_resume: Mapped[bool | None] = mapped_column(Boolean, nullable=True)
    detected_fields: Mapped[dict | None] = mapped_column(
        JSON, nullable=True
    )  # List of detected form fields
    screening_questions: Mapped[list | None] = mapped_column(
        JSON, nullable=True
    )  # Detected screening questions
    analyzed_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True), nullable=True)

    # Soft delete - prevents re-scraping deleted jobs
    deleted_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True), nullable=True)

    # Additional scrape fields from python-jobspy
    is_remote: Mapped[bool | None] = mapped_column(Boolean, nullable=True)
    job_type: Mapped[str | None] = mapped_column(
        String(50), nullable=True
    )  # fulltime, parttime, internship, contract
    company_url: Mapped[str | None] = mapped_column(String(2048), nullable=True)

    # Relationships
    user: Mapped["User"] = relationship("User", lazy="selectin")
    profile: Mapped["JobProfile | None"] = relationship("JobProfile", lazy="selectin")

    @property
    def job_status(self) -> JobStatus:
        """Get status as enum."""
        return JobStatus(self.status)

    def __repr__(self) -> str:
        return f"<Job(id={self.id}, title={self.title}, company={self.company}, score={self.relevance_score})>"
